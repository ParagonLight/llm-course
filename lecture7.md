---
theme: gaia
_class: lead
paginate: true
backgroundColor: #fff
# backgroundImage: url('https://marp.app/assets/hero-background.svg')
marp: true
---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
a[href='red'] {
    color: red;
    pointer-events: none;
    cursor: default;
    text-decoration: none;
}
</style>

<style>
img[alt~="right"] {
  display: block;
  margin:auto;
}
a[href='red'] {
    color: red;
    pointer-events: none;
    cursor: default;
    text-decoration: none;
}
</style>


![bg left:45% 80%](images/course.webp)

# **LLMæ™ºèƒ½åº”ç”¨å¼€å‘**

ç¬¬7è®²: å¤§è¯­è¨€æ¨¡å‹è§£æ IV

åŸºäºHF LlaMAå®ç°çš„è®²è§£

<!-- https://marp.app/ -->

---

# LLMç»“æ„çš„å­¦ä¹ è·¯å¾„

* LLMç»“æ„è§£æ(å¼€æºLlaMA)
* è‡ªå®šä¹‰æ•°æ®é›†æ„é€ 
* è‡ªå®šä¹‰æŸå¤±å‡½æ•°å’Œæ¨¡å‹è®­ç»ƒ/å¾®è°ƒ

---

# Transformerç»å…¸ç»“æ„

<!-- ![bg right:40% 100%](images/l4/transformer.png) -->

![bg right:30% 100%](images/l4/llama_arch_rope.png)

* Encoder-decoderç»“æ„
* è¾“å…¥éƒ¨åˆ†
  * Input embedding
  * Positional embedding
* Transformeréƒ¨åˆ†
  * Feed forward network
  * Attention module
    * **Flash Attention**
  

---

# HF ç›¸å…³å‚è€ƒé“¾æ¥

* [GitHub ä»“åº“](https://github.com/Dao-AILab/flash-attention)ï¼ˆä»“åº“ä¸­åŒ…å« V1ã€V2 çš„è®ºæ–‡ï¼‰
* [HuggingFace](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one)
* [From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)
* [FlashAttention V1 çš„æ¨å¯¼ç»†èŠ‚](https://www.zhihu.com/question/611236756/answer/3132304304)
* [FlashAttention V1ã€V2 å·®å¼‚æ€»ç»“](https://zhuanlan.zhihu.com/p/665170554)



---

# FlashAttention

* **Installation**
* GPU Basics
* FlashAttention V1
* FlashAttention V2
* Other


---

## Note
* ä¸€å®šè¦å…ˆæµè§ˆä¸€é GitHub ä»“åº“ä¸­çš„ Installation and features
* å®‰è£…è¿‡ç¨‹ä¸­ä¼šä½¿ç”¨ ninja åšç¼–è¯‘ï¼Œä¸€å®šè¦æ³¨æ„è®¾ç½® MAX_JOBS ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢æœºå™¨å†…å­˜è¢«å¿«é€Ÿç”¨å®Œç¼–è¯‘è¿‡ç¨‹æ¯”è¾ƒæ…¢ï¼Œè¿™æ˜¯æ­£å¸¸çš„
* FlashAttention ç›®å‰ä»…æ”¯æŒ Ampereã€Adaã€Hopper æ¶æ„çš„ GPU
* FlashAttention ä»…æ”¯æŒ fp16 å’Œ bf16 ä¸¤ç§æ•°æ®ç±»å‹

---

# FlashAttention
* Installation
* **GPU Basics**
* FlashAttention V1
* FlashAttention V2
* Other


---


# GPU Architecture
* ä»æŠ½è±¡çš„è§’åº¦çœ‹ï¼ŒGPU çš„ç»„ä»¶åŒ…æ‹¬ï¼šStreaming Multiprocessorsã€on-chip L2 cacheã€high-bandwidth DRAM
* å…¶ä¸­è®¡ç®—æŒ‡ä»¤é€šè¿‡ SM æ‰§è¡Œï¼Œæ•°æ®å’Œä»£ç ä¼šä» DRAM ç¼“å­˜åˆ° cache
  * ä»¥ A100 ä¸ºä¾‹ï¼ŒåŒ…å« 108 ä¸ª SMã€40MB çš„ L2 cacheã€80G çš„ DRAM

![bg right:30% 100%](images/l7/sm.png)


---

# Streaming Multiprocessorsï¼ˆSMï¼‰
* Streaming Multiprocessorsï¼ˆSMï¼‰ï¼šGPU å†…éƒ¨çš„æ•°æ®å¤„ç†å•å…ƒï¼Œæ¯ä¸ª SM æœ‰è‡ªå·±çš„æ‰§è¡Œæµï¼Œå¯ä»¥ç±»æ¯”ä¸ºå¤šæ ¸ CPU ä¸­çš„ä¸€ä¸ªæ ¸ï¼Œåªæ˜¯ GPU çš„ä¸€ä¸ªæ ¸èƒ½è¿è¡Œå¤šä¸ªçº¿ç¨‹
* ä¸€ä¸ª SM çš„æ„æˆï¼š
  * å¤šä¸ª CUDA Coreï¼Œç”¨äºåšæ•°å­¦è¿ç®—
  * è‹¥å¹² special function unitsï¼Œç”¨äºç‰¹æ®Šçš„è®¡ç®—åœºæ™¯
  * å‡ ä¸ª warp scheduler

---

# Streaming Multiprocessorsï¼ˆSMï¼‰

* æ­¤å¤–ï¼Œä¸€ä¸ª SM è¿˜æ‹¥æœ‰ï¼š
  * ä¸€ä¸ª read-only constant cache
  * ä¸€ä¸ªç»Ÿä¸€çš„ data cache å’Œ shared memoryï¼Œå¤§å°æ ¹æ®å…·ä½“çš„è®¾å¤‡è€Œä¸åŒï¼Œå¤§æ¦‚æ˜¯ä¸€ç™¾å¤šåˆ°ä¸¤ç™¾å¤š KBï¼Œshared memory çš„å¤§å°å¯é…ç½®ï¼Œé…ç½®å®Œåå‰©ä½™çš„å­˜å‚¨ç©ºé—´å°±ä½œä¸º L1 cache


---


# Thread Hierarchy
* å¤šä¸ªçº¿ç¨‹è¢«ç»„ç»‡æˆä¸€ä¸ª blockï¼Œåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼ŒåŒä¸€ä¸ª block å†…çš„çº¿ç¨‹ä¼šè¢«æ”¾åœ¨ä¸€ä¸ª SM ä¸Šæ‰§è¡Œï¼Œå› æ­¤åŒä¸€ä¸ª block ä¸­çš„çº¿ç¨‹ä¼šå…±äº« L1ï¼Œä¸€ä¸ª block ä¸­æœ€å¤šåŒ…å« 1024 ä¸ªçº¿ç¨‹
* å¤šä¸ª block ä¼šè¢«ç»„ç»‡æˆä¸€ä¸ª gridï¼Œä¸€ä¸ª grid ä¸­åŒ…å«å¤šå°‘ block ç”±å…·ä½“çš„æ•°æ®è§„æ¨¡å†³å®š


---

# Thread Hierarchy

* ä¸€æ–¹é¢æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è®©ä¸€æ¬¡è®¡ç®—å°½å¯èƒ½ä½¿ç”¨å¤šä¸ª block æ¥æé«˜å¹¶è¡Œåº¦ï¼›å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è®©ä¸€ä¸ª SM å¹¶å‘æ‰§è¡Œå¤šä¸ªè®¡ç®—ä»»åŠ¡çš„ block
* ä»ç¡¬ä»¶æ‰§è¡Œçš„è§’åº¦æ¥è¯´ï¼ŒSM ä¼šæŠŠä¸€ä¸ª block ä¸­çš„çº¿ç¨‹å†åˆ†æˆ 32 ä¸ªä¸ºä¸€ç»„ï¼Œç§°ä¸º warpï¼Œä¸€ä¸ª warp ä¸Šçš„çº¿ç¨‹ä¼šæ‰§è¡Œå®Œå…¨ä¸€æ ·çš„æŒ‡ä»¤ï¼Œæ‰€ä»¥æ•ˆç‡æœ€é«˜çš„æƒ…å†µæ˜¯ warp ä¸­çš„çº¿ç¨‹æ‰§è¡Œè·¯å¾„å®Œå…¨ç›¸åŒï¼›è€Œå½“å‡ºç°åˆ†æ”¯çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå¯¼è‡´éƒ¨åˆ†çº¿ç¨‹æå‰æ‰§è¡Œå®ŒæŒ‡ä»¤ï¼Œè¿›è€Œå¯¼è‡´å½“å‰çš„ GPU core ç©ºé—²

---

# Memory Hierarchy

![w:600 center](images/l7/mem.png)



---

# Memory Hierarchy

![w:900 center](images/l7/mem_features.png)


---

# Memory Hierarchy

![w:900 center](images/l7/mem_space.png)

---

# Memoryè¡¥å……è¯´æ˜

* on-chip memoryï¼šåŒ…æ‹¬ register å’Œ shared memoryï¼Œæ‰€æœ‰çš„ on-chip memory éƒ½æ˜¯ SRAM
* off-chip memoryï¼šåŒ…æ‹¬ globalã€localã€constantsã€texture memoryï¼Œæ‰€æœ‰çš„ off-chip memory éƒ½æ˜¯ DRAM
* Global Memory ä¸­è®¿é—®çš„æ•°æ®æ€»æ˜¯ä¼šè¢«ç¼“å­˜åˆ° L2 ä¸­ï¼Œå½“æ»¡è¶³ä¸€äº›æ›´ä¸¥æ ¼çš„æ¡ä»¶æ—¶ä¼šè¿›ä¸€æ­¥è¢«ç¼“å­˜åˆ° L1 ä¸­
* GPU DRAM çš„å¤§å° = off-chip memory çš„å¤§å° = "æ˜¾å­˜"

---

# Memoryè¡¥å……è¯´æ˜
* High Bandwidth Memoryï¼ˆHBMï¼‰ï¼šå¯ä»¥è®¤ä¸ºæŒ‡çš„å°±æ˜¯ DRAM
* L1 cache å’Œ shared memory å…±äº«ä¸€å— on-chip memoryï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™ä¸¤è€…çš„è®¿é—®é€Ÿåº¦ç›¸åŒ
  * cache æ˜¯ç¨‹åºå‘˜æ— æ³•æ§åˆ¶çš„ï¼Œä½† shared memory å¯ä»¥


---


# FlashAttention
* Installation
* GPU Basics
* **FlashAttention V1**
* FlashAttention V2
* Other





---

# Basic Info
* æ•ˆæœï¼šFlashAttention å¯ä»¥åŠ é€Ÿ Attention Layer åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—é€Ÿåº¦ï¼Œå¹¶ä¸”ä¿è¯è®¡ç®—ç»“æœå‡†ç¡®
* åŠ¨æœº: Transformer æ¶æ„çš„è®¡ç®—æ—¶é—´å¼€é”€å¤§
* åŸç†ï¼šå‡å°‘å­˜å‚¨è®¿é—®å¼€é”€ï¼Œè¿™ä¸ç»å¤§æ•°å‡å°‘è®¡ç®—æ—¶é—´å¤æ‚åº¦æ–¹æ³•çš„åŸç†æ˜¯ä¸ä¸€æ ·çš„



---

# æ ‡å‡† Self Attention


![w:950 center](images/l7/self_attn.png)

* åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€å…±åŒ…å«äº† 8 æ¬¡éœ€è¦è®¿é—® HBM çš„æ“ä½œ
  * ç¬¬ 1 è¡Œï¼šè¯» Qã€Kï¼Œå†™ S
  * ç¬¬ 2 è¡Œï¼šè¯» Sï¼Œå†™ P
  * ç¬¬ 3 è¡Œï¼šè¯» Pã€Vï¼Œå†™ O
* HBM è®¿é—®æˆæœ¬ï¼š $ğ‘¶(ğ‘ğ‘‘+ğ‘^2)$ï¼Œ$ğ‘$ è¡¨ç¤ºseq_lenï¼Œ $ğ‘‘$ è¡¨ç¤º head_dim


---

# ä¼˜åŒ–ç»´åº¦

![w:950 center](images/l7/self_attn.png)

* ä¸€ç§æ€è·¯æ˜¯ï¼šå‡å°‘æ¯ä¸€æ­¥ä¸­å®é™…è®¿é—® HBMï¼ˆglobal memoryï¼‰çš„æ¬¡æ•°
* æˆ–è€…ï¼šè°ƒæ•´ç®—æ³•æ­¥éª¤ï¼Œå‡å°‘æ•´ä½“æµç¨‹ä¸Šè®¿é—® HBM çš„æ¬¡æ•°

---


# ä» block å‡ºå‘æ€è€ƒé—®é¢˜
* ä»¥çŸ©é˜µä¹˜æ³• ğ‘ª=ğ‘¨Ã—ğ‘© ä¸ºä¾‹ï¼Œåœ¨å®é™…çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œçº¿ç¨‹ä¼šè¢«ç»„ç»‡æˆ blockï¼Œå†äº¤ç”± SM æ‰§è¡Œ
* ä»¥ ğ‘ª ä¸º 32\*32 çš„çŸ©é˜µï¼Œblock ä¸º 16\*16 ä¸ºä¾‹ï¼Œä¸€ç§æœ´ç´ çš„å®ç°æ–¹æ³•ï¼š
![w:800 center](images/l7/matmul.png)
* C ä¸­æ¯ä¸ªä½ç½®çš„è®¡ç®—éœ€è¦è®¿é—® global memory 2\*32 æ¬¡ï¼Œæ€»å…± 2\*32\*32\*32 æ¬¡



---

## Tiling æŠ€æœ¯
* åœ¨æœ´ç´ çš„å®ç°æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è€ƒè™‘åˆ©ç”¨ shared memoryï¼Œè€Œ Tiling æŠ€æœ¯é€šè¿‡åˆ©ç”¨ shared memory å‡å°‘ global memory çš„è®¿é—®
![w:800 center](images/l7/tiling.png)
* $ğ‘¨_{ğŸ,ğŸ}Ã—ğ‘©_{ğŸ,ğŸ}+ğ‘¨_{ğŸ,ğŸ}Ã—ğ‘©_{ğŸ,ğŸ}=ğ‘ª_{ğŸ,ğŸ}$
* $ğ‘¨_{ğŸ,ğŸ}$ å’Œ $ğ‘©_{ğŸ,ğŸ}$ å¯ä»¥åŒæ—¶å­˜å‚¨åœ¨ shared memory ä¸Šï¼Œ $ğ‘ª_{ğŸ,ğŸ}$ ä¸­çš„æ¯ä¸ªå…ƒç´ çš„å€¼å­˜å‚¨åœ¨ register ä¸Š


---

## Tiling æŠ€æœ¯ (cont'd)
* ç¬¬ä¸€è½®è¿­ä»£å­˜å‚¨è§’åº¦å›¾ç¤ºï¼š
![w:800 center](images/l7/tiling1.png)


---

## Tiling æŠ€æœ¯ (cont'd)
* ç¬¬äºŒè½®è¿­ä»£å­˜å‚¨è§’åº¦å›¾ç¤ºï¼š
![w:800 center](images/l7/tiling2.png)


---

# Tiling æŠ€æœ¯ (cont'd)
* æ€»è®¡ç®—é‡ä¿æŒä¸å˜
* ä½†æ˜¯æ€»çš„ global memory çš„è®¿é—®æ¬¡æ•°å¤§å¤§é™ä½ï¼Œæˆ‘ä»¬ç®—å‡º C çŸ©é˜µå››åˆ†ä¹‹ä¸€çš„ç»“æœæ—¶ï¼Œè®¿é—®äº† 16\*16\*4 æ¬¡ global memoryï¼Œé‚£ä¹ˆæ€»å…±å°†è®¿é—® 16\*16\*4\*4 æ¬¡ï¼Œä¸€å…± 4096 æ¬¡ï¼›è€Œä¹‹å‰ naive çš„æ–¹æ³•è®¿é—®äº† 65536 æ¬¡ï¼Œå‡å°‘ä¸ºäº†åŸæ¥çš„ 1/16
* è°ƒæ•´ block çš„å¤§å°è¿˜å¯ä»¥è¿›ä¸€æ­¥æ”¹å˜ global memory çš„è®¿é—®æ¬¡æ•°



---

# Unfortunately
* Tiling æŠ€æœ¯è™½ç„¶å¯ç”¨äºçŸ©é˜µä¹˜æ³•ï¼Œä½†ä¸èƒ½ç›´æ¥ç”¨äº Attention çš„è®¡ç®—
  * åœ¨ Attention Layer çš„è®¡ç®—ä¸­ï¼Œå­˜åœ¨ä¸€æ¬¡ row-wise softmax æ“ä½œ
![w:200 center](images/l7/softmax_c.png)
* åœ¨ä»…è®¡ç®—å‡º $ğ‘ª_{ğŸ,ğŸ}$ çš„æƒ…å†µä¸‹ï¼Œæ— æ³•è®¡ç®— softmax çš„å€¼ï¼Œå› ä¸º softmax çš„å€¼è¿˜ä¾èµ–äº $ğ‘ª_{ğŸ,ğŸ}$

---

# Unfortunately


* å› æ­¤ Tiling æŠ€æœ¯ä»…ä»…å‡å°‘äº†æ ‡å‡† Attention ç®—æ³•ä¸­çŸ©é˜µä¹˜æ³•çš„å®é™… global memory è®¿é—®æ¬¡æ•°ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰ä»æ•´ä½“ä¸Šæ”¹å˜æ ‡å‡† Attention ç®—æ³•çš„æµç¨‹
![w:200 center](images/l7/softmax_c.png)



---

# Safe Softmax
Softmax çš„å…¬å¼ï¼š
![w:600 center](images/l7/softmax_eq.png)
ä¸ºäº†é˜²æ­¢æŒ‡æ•°çˆ†ç‚¸é—®é¢˜ï¼Œåœ¨å®é™…è®¡ç®—çš„æ—¶å€™ä¼šé‡‡ç”¨ Safe Softmaxï¼š
![w:300 center](images/l7/safe_softmax.png)
ä¸€èˆ¬æ¥è¯´ï¼Œä¸Šè¿°å…¬å¼ä¸­ $ğ‘š=\max_{ğ‘—=1}^ğ‘ (ğ‘¥_ğ‘—)$ï¼Œä»è€Œä¿è¯æŒ‡æ•°é¡¹<=0


---

## ä¸€ç§è¿­ä»£å¼çš„ Safe Softmax çš„ç®—æ³•ï¼ˆV1ï¼‰

![w:850 center](images/l7/safe_softmax_alg.png)

---

## Online Softmaxï¼ˆV2ï¼‰
* ä¼˜åŒ–æ€è·¯ï¼šæ¶ˆé™¤ $ğ‘‘_ğ‘–$ å¯¹ $ğ‘š_ğ‘$ çš„ä¾èµ–
![w:500 center](images/l7/online_softmax.png)

---

## Online Softmaxï¼ˆV2ï¼‰ 

V2ç‰ˆæœ¬ç®—æ³•
![w:800 center](images/l7/online_softmax_v2.png)

---

# Again, Unfortunately
* ä»¥ä¸Šä¼˜åŒ–å¯¹äº softmax æ“ä½œæ¥è¯´å·²ç»åˆ°å¤´äº†ï¼Œæˆ‘ä»¬ä¸å¯èƒ½åœ¨ä¸€æ¬¡å¾ªç¯ä¸­æŠŠ softmax çš„ç»“æœè®¡ç®—å‡ºæ¥
  * åŸå› ï¼šå‘é‡ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸å¯èƒ½åœ¨æ²¡æœ‰éå†åˆ°åç»­å…ƒç´ çš„æƒ…å†µä¸‹ï¼Œç¡®å®šå½“å‰å…ƒç´ æœ€ç»ˆçš„ softmax å€¼

# But
* Attention Layer çš„æœ€ç»ˆç›®çš„å¹¶ä¸æ˜¯ä¸ºäº†è®¡ç®— softmaxï¼Œè€Œæ˜¯ softmax ä»¥åçš„è¿˜éœ€è¦ä¹˜ä»¥çŸ©é˜µ Vï¼Œ**å¾—åˆ°æœ€ç»ˆçš„è¾“å‡º**


---

## ä¸€ç§ 2-pass çš„ Self Attention çš„ç®—æ³•ï¼ˆV1ï¼‰
![w:700 center](images/l7/flashattention_v1.png)

---

## æ”¹è‰¯ç‰ˆçš„ 1-pass ç®—æ³•ï¼ˆV2ï¼‰
![w:700 center](images/l7/flash_attn_v1_1pass.png)

---

## æ”¹è‰¯ç‰ˆçš„ 1-pass ç®—æ³•ï¼ˆV2ï¼‰ï¼ˆcont'dï¼‰

![w:700 center](images/l7/flash_attn_1pass.png)
* è™½ç„¶ softmax æ— æ³•ç”¨ 1-pass çš„æ–¹å¼è§£å†³ï¼Œä½†æ˜¯ Self Attention çš„è®¡ç®—å¯ä»¥ç”¨1-passçš„æ–¹å¼è§£å†³
  * ä»¥ä¸Š1-pass Self Attention ç®—æ³•å¯çœ‹ä½œ FlashAttention V1 çš„åŸå‹


---

## FlashAttention V1
* FlashAttention åœ¨å®ç°æ—¶ï¼Œè¿˜è€ƒè™‘åˆ°äº† Tiling æŠ€æœ¯
![w:900 center](images/l7/flash_attn_v1_tiling.png)


---

## FlashAttention V1
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè“è‰²çš„éƒ¨åˆ†è¡¨ç¤ºå½“å‰å­˜å‚¨åœ¨ shared memory ä¸­çš„éƒ¨åˆ†
![w:400 center](images/l7/flash_attn_share.png)
FlashAttention çš„å®ç°æ˜¯ä¸å”¯ä¸€çš„ï¼Œäº‹å®ä¸Šï¼Œå¾ˆå¤šå®ç°éƒ½æ²¡æœ‰å®Œå…¨é‡‡ç”¨åŸå§‹è®ºæ–‡ä¸­çš„æ–¹æ³•ï¼Œä¼šæœ‰ä¸€å®šç¨‹åº¦çš„è°ƒæ•´


---

# FlashAttention
* Installation
* GPU Basics
* FlashAttention V1
* **FlashAttention V2**
* Other


---

# æ”¹è¿›ä¸€ï¼šè°ƒæ•´å†…å¤–å¾ªç¯
* FlashAttention V1 ä¸­é‡‡ç”¨äº†ä¸€ä¸ªéç›´è§‰çš„å¤–å±‚å¾ªç¯çŸ©é˜µ ğ¾,ğ‘‰ï¼Œå†…å±‚å¾ªç¯çŸ©é˜µ ğ‘„,ğ‘‚ çš„æ–¹å¼ï¼Œè¿™ä¼šå¯¼è‡´çŸ©é˜µ ğ‘‚ è¢«é¢å¤–åŠ è½½
* äº‹å®ä¸Šï¼Œåœ¨ FlashAttention V2 å‡ºæ¥ä¹‹å‰ï¼Œå¾ˆå¤š FlashAttention çš„å®ç°å°±ä¿®æ”¹äº†è¿™ä¸ªå¾ªç¯é¡ºåº

![bg right:35% 100%](images/l7/flash_attn_share.png)


---

# æ”¹è¿›äºŒï¼šå‡å°‘äº†éçŸ©é˜µä¹˜æ³•çš„è¿ç®—æ¬¡æ•°
* ç°ä»£ GPU å¯¹çŸ©é˜µä¹˜æ³•æœ‰ä¸“é—¨çš„ç¡¬ä»¶ä¼˜åŒ–ï¼ŒçŸ©é˜µä¹˜æ³•flopæ˜¯éçŸ©é˜µä¹˜æ³•flopçš„16å€å·¦å³
  * å…·ä½“å®ç°ä¸Šï¼ŒFlashAttention V1 æ¯è½®è¿­ä»£éƒ½æœ‰ä¸€ä¸ª rescale æ“ä½œï¼š
![w:800 center](images/l7/flash_attn_rescale.png)
* åœ¨ V2 ä¸­ï¼Œä¸å†åœ¨æ¯è½®è¿­ä»£ä¸­éƒ½é™¤ä»¥$ğ‘‘_ğ‘–^â€²$ï¼Œè€Œæ˜¯ç­‰å¾ªç¯ä½“ç»“æŸä»¥åï¼Œå¯¹è®¡ç®—å¾—åˆ°çš„ $ğ’_ğ‘^â€²$ ç»Ÿä¸€é™¤ä»¥ $ğ‘‘_ğ‘^â€²$


---

# æ”¹è¿›ä¸‰ï¼šWarp Level å¹¶è¡Œåº¦
å‡è®¾ä¸€ä¸ª block å®é™…ä¸Šä¼šè¢« SM åˆ’åˆ†æˆ 4 ä¸ª warpï¼Œåœ¨ V1 ç‰ˆæœ¬ä¸­ï¼ŒçŸ©é˜µ ğ¾,ğ‘‰ çš„ block ä¼šè¢«åˆ’åˆ†æˆ 4 ä¸ª warpï¼Œæ¯ä¸ª warp è®¡ç®— $ğ‘¸_ğ‘– ğ‘²_ğ‘—^ğ‘‡$ åä¼šå¾—åˆ°ä¸€ä¸ª $ğµ_ğ‘ŸÃ—\frac{ğµ_ğ‘}{4}$ çš„çŸ©é˜µï¼Œéœ€è¦ 4 ä¸ª warp å…¨éƒ¨è®¡ç®—å®Œä»¥åï¼ŒæŠŠå››ä¸ªçŸ©é˜µæ’æˆä¸€è¡Œï¼ˆä¸‹å›¾ä¸­ V1 ç‰ˆæœ¬çº¢è‰²çš„å››ä¸ªçŸ©é˜µï¼‰ï¼Œæ‰èƒ½è®¡ç®— $ğ‘¸_ğ‘– ğ‘²_ğ‘—^ğ‘‡$ çœŸæ­£çš„å€¼ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­å­˜åœ¨ warp ä¹‹é—´çš„é€šä¿¡
![w:800 center](images/l7/flash_attn_v1_v2.png)

---

# æ”¹è¿›ä¸‰ï¼šWarp Level å¹¶è¡Œåº¦ï¼ˆcont'dï¼‰
åœ¨ V2 ç‰ˆæœ¬ä¸­ï¼ŒçŸ©é˜µ ğ‘„ çš„ block ä¼šè¢«åˆ’åˆ†æˆ 4 ä¸ª warpï¼Œè¿™ç§æƒ…å†µä¸‹æ¯ä¸ª warp è®¡ç®—å‡ºæ¥çš„ç»“æœå°±æ˜¯ä¸€ä¸ª $\frac{ğµ_ğ‘Ÿ}{4}Ã—ğµ_ğ‘$ çš„çŸ©é˜µï¼Œè¿™ä¸ªçŸ©é˜µå·²ç»åŒ…å«äº† $ğ‘¸_ğ‘– ğ‘²_ğ‘—^ğ‘‡$ ä¸­å®Œæ•´çš„ $\frac{ğµ_ğ‘Ÿ}{4}$ è¡Œï¼Œæ‰€ä»¥æ•´ä¸ªè®¡ç®—å°±åªéœ€è¦åœ¨ warp å†…éƒ¨è¿›è¡Œï¼Œä¸éœ€è¦è¿›è¡Œ warp ä¹‹é—´çš„é€šä¿¡

![w:800 center](images/l7/flash_attn_v1_v2.png)


---


# FlashAttention
* Installation
* GPU Basics
* FlashAttention V1
* FlashAttention V2
* **Other**

---

# FlashAttention ä½¿ç”¨é€”å¾„
* ä½¿ç”¨å®˜æ–¹åº“ flash_attnï¼Œå¯ä»¥é€šè¿‡ pip ç›´æ¥å®‰è£…ï¼Œè¿™ç§æ–¹æ³•å¦‚æœéœ€è¦åšä¸€äº›é€»è¾‘ä¸Šçš„ä¿®æ”¹ï¼ˆä¾‹å¦‚åŠ  maskï¼‰ï¼Œå­¦ä¹ å’Œ Debug çš„æˆæœ¬è¾ƒé«˜
* ä½¿ç”¨ Triton Language ä¸­çš„å®ç°ï¼Œå®é™…æ€§èƒ½ä¹Ÿå¾ˆå¥½

